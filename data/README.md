# Data Folder - README

## Overview

This folder contains all data files used in the ML pipeline for pod forecasting. Data flows through multiple stages from raw ingestion to cleaned datasets ready for training.

---

## Data Flow

```
Google Sheets
    v
historical_raw.csv      <- Raw data from "Historical" sheet
budget_raw.csv          <- Raw data from "Budget" sheet
    v
[Data Cleaning]
    v
training_clean.csv      <- Cleaned historical data (for training)
budget_clean.csv        <- Cleaned budget data (for prediction)
    v
[Model Training]
    v
Models trained (stored in ../models/)
```

---

## File Descriptions

### Raw Data Files (From Google Sheets)

#### `historical_raw.csv`
**Source:** Google Sheets - "Historical" tab  
**Generated by:** `fetch_from_sheets.py`  
**Purpose:** Raw historical data with known pod counts (training data)

**Columns:**
- `date` - Date in DD/MM/YYYY format (e.g., 15/07/2024)
- `gmv` - Gross Merchandise Value (may have commas: 18,953,653.00)
- `users` - Number of active users (may have commas: 74,540)
- `marketing_cost` - Marketing spend (may have commas: 148,377.00)
- `fe_pods` - Frontend pods used (actual values)
- `be_pods` - Backend pods used (actual values)

**Example:**
```csv
date,gmv,users,marketing_cost,fe_pods,be_pods
15/07/2024,18953653,74540,148377,20,9
16/07/2024,19364132,75321,151234,20,9
17/07/2024,18742901,73456,145678,20,9
```

**Characteristics:**
- Contains actual pod counts (labels)
- Used for model training
- Typically 15-30 days of data
- Numbers may have commas (cleaned later)
- Dates in DD/MM/YYYY format

---

#### `budget_raw.csv`
**Source:** Google Sheets - "Budget" tab  
**Generated by:** `fetch_from_sheets.py`  
**Purpose:** Future dates with budget data but no pod counts (prediction data)

**Columns:**
- `date` - Date in DD/MM/YYYY format
- `gmv` - Projected GMV
- `users` - Projected users
- `marketing_cost` - Projected marketing spend

**Example:**
```csv
date,gmv,users,marketing_cost
01/08/2024,9744406,74410,169148
02/08/2024,12456789,78230,172345
03/08/2024,11234567,76890,168234
```

**Characteristics:**
- NO pod counts (we predict these)
- Future dates
- Rest of the year (300+ days)
- Numbers may have commas
- Dates in DD/MM/YYYY format

---

### Cleaned Data Files (After Processing)

#### `training_clean.csv`
**Source:** `historical_raw.csv`  
**Generated by:** `clean_data.py`  
**Purpose:** Cleaned and validated training data

**Columns:**
- `date` - Date as datetime (YYYY-MM-DD format)
- `gmv` - Numeric (commas removed)
- `users` - Numeric (commas removed)
- `marketing_cost` - Numeric (commas removed)
- `fe_pods` - Integer
- `be_pods` - Integer

**Example:**
```csv
date,gmv,users,marketing_cost,fe_pods,be_pods
2024-07-15,18953653.0,74540,148377.0,20,9
2024-07-16,19364132.0,75321,151234.0,20,9
2024-07-17,18742901.0,73456,145678.0,20,9
```

**Characteristics:**
- Dates parsed to datetime
- Commas removed from numbers
- No missing values
- No duplicates
- Sorted by date
- Ready for model training

---

#### `budget_clean.csv`
**Source:** `budget_raw.csv`  
**Generated by:** `clean_data.py`  
**Purpose:** Cleaned budget data for predictions

**Columns:**
- `date` - Date as datetime
- `gmv` - Numeric (commas removed)
- `users` - Numeric (commas removed)
- `marketing_cost` - Numeric (commas removed)

**Example:**
```csv
date,gmv,users,marketing_cost
2024-08-01,9744406.0,74410,169148.0
2024-08-02,12456789.0,78230,172345.0
2024-08-03,11234567.0,76890,168234.0
```

**Characteristics:**
- Dates parsed to datetime
- Commas removed from numbers
- No missing values
- No duplicates
- Sorted by date
- Ready for batch predictions

---

## Data Quality Checks

The cleaning process (`clean_data.py`) performs:

1. **Date Parsing**
   - Converts DD/MM/YYYY to datetime
   - Validates date ranges
   - Checks for invalid dates

2. **Comma Removal**
    - Removes thousand separators
    - `"18,953,653.00"` -> `18953653.0`

3. **Type Conversion**
   - Converts strings to numeric types
   - Validates data types

4. **Missing Values**
   - Detects null/empty values
   - Reports which columns affected
   - Drops rows with missing data

5. **Duplicates**
   - Identifies duplicate rows
   - Removes duplicates
   - Reports count removed

6. **Outlier Detection**
   - Checks values against thresholds
   - Reports suspicious values
   - Logs warnings

---

## File Lifecycle

### 1. Initial Setup (First Time)

```bash
# Empty data folder
data/
└── .gitkeep
```

### 2. After Data Ingestion

```bash
# Run: python fetch_from_sheets.py
data/
├── historical_raw.csv    # 30 rows
└── budget_raw.csv        # 334 rows
```

### 3. After Data Cleaning

```bash
# Run: python clean_data.py
data/
├── historical_raw.csv
├── budget_raw.csv
├── training_clean.csv    # Cleaned historical
└── budget_clean.csv      # Cleaned budget
```

### 4. Daily Updates (Cronjob)

```bash
# Cronjob runs daily, overwrites files
data/
├── historical_raw.csv    # Latest from Sheets
├── budget_raw.csv        # Latest from Sheets
├── training_clean.csv    # Cleaned version
└── budget_clean.csv      # Cleaned version
```

---

## Data Statistics

### Typical Data Volume

| File | Rows | Size | Update Frequency |
|------|------|------|------------------|
| `historical_raw.csv` | ~30 | ~5 KB | Daily |
| `budget_raw.csv` | ~334 | ~20 KB | Daily |
| `training_clean.csv` | ~30 | ~5 KB | After cleaning |
| `budget_clean.csv` | ~334 | ~20 KB | After cleaning |

### Expected Ranges

**GMV:**
- Min: ~8,000,000 (8M)
- Max: ~20,000,000 (20M)
- Normal range: 8M - 20M

**Users:**
- Min: ~70,000
- Max: ~90,000
- Normal range: 70K - 90K

**Marketing Cost:**
- Min: ~100,000
- Max: ~200,000
- Normal range: 100K - 200K

**Pods (Historical only):**
- Frontend: 10 or 20
- Backend: 4 or 9
- Pattern: Two operating modes

---

## Date Formats

### In Raw Files (from Google Sheets)
```
DD/MM/YYYY
15/07/2024
01/08/2024
```

### In Cleaned Files
```
YYYY-MM-DD
2024-07-15
2024-08-01
```

### Important Notes
- Google Sheets exports DD/MM/YYYY (European format)
- Cleaning converts to YYYY-MM-DD (ISO format)
- Both represent the same dates
- 15/07/2024 = 2024-07-15 = July 15, 2024

---

## Usage Examples

### Load Training Data

```python
import pandas as pd

# Load cleaned training data
df = pd.read_csv('data/training_clean.csv', parse_dates=['date'])

print(f"Training data shape: {df.shape}")
print(f"Date range: {df['date'].min()} to {df['date'].max()}")
print(f"Columns: {list(df.columns)}")
```

### Load Budget Data

```python
# Load cleaned budget data
budget = pd.read_csv('data/budget_clean.csv', parse_dates=['date'])

print(f"Budget data shape: {budget.shape}")
print(f"Dates to predict: {len(budget)}")
```

### Check Data Quality

```python
# Check for missing values
print(df.isnull().sum())

# Check data types
print(df.dtypes)

# Check value ranges
print(df[['gmv', 'users', 'marketing_cost']].describe())
```

---

## Common Issues

### Issue 1: Commas in Numbers

**Symptom:** Numbers appear as strings, can't do math
```python
df['gmv']  # "18,953,653.00" (string)
```

**Cause:** Google Sheets exports with thousand separators

**Solution:** Run `clean_data.py` to remove commas
```python
df['gmv']  # 18953653.0 (float)
```

---

### Issue 2: Wrong Date Format

**Symptom:** Dates parsed incorrectly
```python
# 15/07/2024 interpreted as July 15 or Feb 15?
```

**Solution:** `clean_data.py` uses `dayfirst=True`
```python
pd.to_datetime(df['date'], dayfirst=True)
# Correctly interprets 15/07/2024 as July 15
```

---

### Issue 3: Missing Values

**Symptom:** Some cells empty in Google Sheets

**Solution:** `clean_data.py` detects and reports
```
Found 5 missing values
  gmv: 2 missing values
  users: 3 missing values
  Dropped 5 rows with missing values
```

---

### Issue 4: Duplicates

**Symptom:** Same date appears twice

**Solution:** `clean_data.py` removes duplicates
```
Found duplicate rows
  Removed 2 duplicate rows
```

---

## Data Validation

### Automated Checks (in clean_data.py)

```python
# Required columns present
assert all(col in df.columns for col in ['date', 'gmv', 'users', 'marketing_cost'])

# No negative values
assert (df['gmv'] >= 0).all()
assert (df['users'] >= 0).all()
assert (df['marketing_cost'] >= 0).all()

# Dates are valid
assert df['date'].dtype == 'datetime64[ns]'

# No duplicates
assert df.duplicated().sum() == 0

# No missing values
assert df.isnull().sum().sum() == 0
```

---

## Backup Strategy

### Manual Backup

```bash
# Backup before updates
cp -r data/ data_backup_$(date +%Y%m%d)/

# Restore if needed
cp -r data_backup_20241005/ data/
```

### Automated Backup (in Cronjob)

```bash
#!/bin/bash
# Before fetching new data
if [ -f "data/historical_raw.csv" ]; then
    mkdir -p backups/$(date +%Y%m%d)
    cp data/*.csv backups/$(date +%Y%m%d)/
fi

# Then fetch new data
python fetch_from_sheets.py
```

---

## Git Considerations

### .gitignore

```gitignore
# Ignore all CSV files (contain sensitive data)
data/*.csv

# Keep folder structure
!data/.gitkeep

# Or commit only sample data
!data/*_sample.csv
```

### Sample Data (for testing)

Create anonymized samples:
```bash
# Create sample with first 5 rows
head -6 data/training_clean.csv > data/training_sample.csv

# Or with fake data
cat > data/training_sample.csv << 'EOF'
date,gmv,users,marketing_cost,fe_pods,be_pods
2024-07-15,18953653,74540,148377,20,9
2024-07-16,19364132,75321,151234,20,9
EOF
```

---

## Monitoring

### Check Data Freshness

```bash
# When was data last updated?
ls -lh data/*.csv

# Check file sizes
du -h data/*.csv

# Count rows
wc -l data/*.csv
```

### Automated Checks

```python
import pandas as pd
from datetime import datetime, timedelta

# Check if data is stale
df = pd.read_csv('data/historical_raw.csv', parse_dates=['date'])
latest_date = df['date'].max()
days_old = (datetime.now().date() - latest_date.date()).days

if days_old > 7:
    print(f"WARNING: Data is {days_old} days old!")
```

---

## Summary

**Data Files:**
- `historical_raw.csv` - Raw training data from Sheets
- `budget_raw.csv` - Raw prediction data from Sheets
- `training_clean.csv` - Cleaned training data
- `budget_clean.csv` - Cleaned prediction data

**Key Points:**
- Raw files have commas, DD/MM/YYYY dates
- Clean files are numeric, YYYY-MM-DD dates
- Cleaning removes commas, validates data, handles missing values
- Files updated daily via cronjob
- Historical data has pod counts, budget data doesn't

**Pipeline:**
```
Google Sheets -> fetch -> raw CSV -> clean -> clean CSV -> train -> model
```

**All data processing is logged in `logs/` directory!**